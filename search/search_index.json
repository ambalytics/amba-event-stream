{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-event-stream amba analytics event stream python package that connects to kafka to produce/connect or process events. Installation pip install amba-event-stream Releasing Releases are published automatically when a tag is pushed to GitHub. # Set next version number export RELEASE=x.x.x # Create tags git commit --allow-empty -m \"Release $RELEASE\" git tag -a $RELEASE -m \"Version $RELEASE\" # Push git push upstream --tags","title":"Home"},{"location":"#amba-event-stream","text":"amba analytics event stream python package that connects to kafka to produce/connect or process events.","title":"amba-event-stream"},{"location":"#installation","text":"pip install amba-event-stream","title":"Installation"},{"location":"#releasing","text":"Releases are published automatically when a tag is pushed to GitHub. # Set next version number export RELEASE=x.x.x # Create tags git commit --allow-empty -m \"Release $RELEASE\" git tag -a $RELEASE -m \"Version $RELEASE\" # Push git push upstream --tags","title":"Releasing"},{"location":"event_ref/","text":"event object Event a representation of an json event to use from_json ( self , json_msg ) set this event from json_msg Parameters: Name Type Description Default json_msg loaded json required Source code in event_stream/event.py def from_json ( self , json_msg ): \"\"\"set this event from json_msg Arguments: json_msg: loaded json \"\"\" self . data = json_msg # self.data = json.loads(json_msg) get ( self , key ) this will get a value to a given key in the data of this event this is equal to data['key'] if access to nested properties is needed use data directly Parameters: Name Type Description Default key a valid key for the data of this event required Source code in event_stream/event.py def get ( self , key ): \"\"\"this will get a value to a given key in the data of this event this is equal to data['key'] if access to nested properties is needed use data directly Arguments: key: a valid key for the data of this event \"\"\" return self . data [ key ] # t = self.data # # todo check if key exist # for key in keys: # t = t[key] # return t get_json ( self ) return this event as json equal to json.dumps(data) Source code in event_stream/event.py def get_json ( self ): \"\"\"return this event as json equal to json.dumps(data) \"\"\" return json . dumps ( self . data ) set ( self , key , value ) this will set a value to a given key in the data of this event this is equal to data['key'] = value if setting nested properties use data directly Parameters: Name Type Description Default key a valid key for the data of this event required value the value to store required Source code in event_stream/event.py def set ( self , key , value ): \"\"\"this will set a value to a given key in the data of this event this is equal to data['key'] = value if setting nested properties use data directly Arguments: key: a valid key for the data of this event value: the value to store \"\"\" self . data [ key ] = value","title":"event"},{"location":"event_ref/#event_stream.event.Event","text":"a representation of an json event to use","title":"Event"},{"location":"event_ref/#event_stream.event.Event.from_json","text":"set this event from json_msg Parameters: Name Type Description Default json_msg loaded json required Source code in event_stream/event.py def from_json ( self , json_msg ): \"\"\"set this event from json_msg Arguments: json_msg: loaded json \"\"\" self . data = json_msg # self.data = json.loads(json_msg)","title":"from_json()"},{"location":"event_ref/#event_stream.event.Event.get","text":"this will get a value to a given key in the data of this event this is equal to data['key'] if access to nested properties is needed use data directly Parameters: Name Type Description Default key a valid key for the data of this event required Source code in event_stream/event.py def get ( self , key ): \"\"\"this will get a value to a given key in the data of this event this is equal to data['key'] if access to nested properties is needed use data directly Arguments: key: a valid key for the data of this event \"\"\" return self . data [ key ] # t = self.data # # todo check if key exist # for key in keys: # t = t[key] # return t","title":"get()"},{"location":"event_ref/#event_stream.event.Event.get_json","text":"return this event as json equal to json.dumps(data) Source code in event_stream/event.py def get_json ( self ): \"\"\"return this event as json equal to json.dumps(data) \"\"\" return json . dumps ( self . data )","title":"get_json()"},{"location":"event_ref/#event_stream.event.Event.set","text":"this will set a value to a given key in the data of this event this is equal to data['key'] = value if setting nested properties use data directly Parameters: Name Type Description Default key a valid key for the data of this event required value the value to store required Source code in event_stream/event.py def set ( self , key , value ): \"\"\"this will set a value to a given key in the data of this event this is equal to data['key'] = value if setting nested properties use data directly Arguments: key: a valid key for the data of this event value: the value to store \"\"\" self . data [ key ] = value","title":"set()"},{"location":"event_stream_base_ref/","text":"EventStreamBase a base class for connecting to kafka build_topic_list ( self ) build a list of topics from the configs Source code in event_stream/event_stream_base.py def build_topic_list ( self ): \"\"\"build a list of topics from the configs \"\"\" result = [] for c_state in self . config_states : result . append ( self . build_topic_name ( c_state )) # print(c_state) # print(self.config_states[c_state]) if 'own_topic' in self . config_states [ c_state ]: for c_o_topic in self . config_states [ c_state ][ 'own_topic' ]: result . append ( self . build_topic_name ( c_state , c_o_topic )) self . topics = result logging . debug ( \" %s current topics for events: %s \" % ( self . log , self . topics )) return result build_topic_name ( self , state , relation_type = '' ) build the name of the topic for a given state Parameters: Name Type Description Default state the state to get the topic for required relation_type optional, in case it has it's own topic '' Source code in event_stream/event_stream_base.py def build_topic_name ( self , state , relation_type = '' ): \"\"\"build the name of the topic for a given state Arguments: state: the state to get the topic for relation_type: optional, in case it has it's own topic \"\"\" result = self . event_string + self . state_separator + state if relation_type != '' : result = result + self . relation_type_separator + relation_type return result get_topic_name ( self , state , relation_type = '' ) get the name of the topic for a given state Parameters: Name Type Description Default state the state to get the topic for required relation_type optional, in case it has it's own topic '' Source code in event_stream/event_stream_base.py def get_topic_name ( self , state , relation_type = '' ): \"\"\"get the name of the topic for a given state Arguments: state: the state to get the topic for relation_type: optional, in case it has it's own topic \"\"\" result = self . event_string + self . state_separator + state # if a relation type is set and has is own topic if relation_type != '' and 'own_topic' in self . config_states [ state ] and relation_type in \\ self . config_states [ state ][ 'own_topic' ]: result = result + self . relation_type_separator + relation_type return result get_topic_name_event ( self , event ) this will resolve an event to it's respected kafka topic Parameters: Name Type Description Default key the event to be resolved required Source code in event_stream/event_stream_base.py def get_topic_name_event ( self , event ): \"\"\"this will resolve an event to it's respected kafka topic Arguments: key: the event to be resolved \"\"\" state = event . get ( 'state' ) relation_type = event . get ( 'relation_type' ) return self . get_topic_name ( state , relation_type ) resolve_event ( self , event ) this will resolve an event to it's respected kafka topic Parameters: Name Type Description Default key the event to be resolved required Source code in event_stream/event_stream_base.py def resolve_event ( self , event ): \"\"\"this will resolve an event to it's respected kafka topic Arguments: key: the event to be resolved \"\"\" topic_name = self . build_topic_name ( event [ 'state' ], event [ 'relation_type' ]) if topic_name in self . topics : return topic_name logging . warning ( self . log + \"Unable to resolve event, topic_name %s not found\" % topic_name ) return False","title":"event stream base"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase","text":"a base class for connecting to kafka","title":"EventStreamBase"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase.build_topic_list","text":"build a list of topics from the configs Source code in event_stream/event_stream_base.py def build_topic_list ( self ): \"\"\"build a list of topics from the configs \"\"\" result = [] for c_state in self . config_states : result . append ( self . build_topic_name ( c_state )) # print(c_state) # print(self.config_states[c_state]) if 'own_topic' in self . config_states [ c_state ]: for c_o_topic in self . config_states [ c_state ][ 'own_topic' ]: result . append ( self . build_topic_name ( c_state , c_o_topic )) self . topics = result logging . debug ( \" %s current topics for events: %s \" % ( self . log , self . topics )) return result","title":"build_topic_list()"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase.build_topic_name","text":"build the name of the topic for a given state Parameters: Name Type Description Default state the state to get the topic for required relation_type optional, in case it has it's own topic '' Source code in event_stream/event_stream_base.py def build_topic_name ( self , state , relation_type = '' ): \"\"\"build the name of the topic for a given state Arguments: state: the state to get the topic for relation_type: optional, in case it has it's own topic \"\"\" result = self . event_string + self . state_separator + state if relation_type != '' : result = result + self . relation_type_separator + relation_type return result","title":"build_topic_name()"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase.get_topic_name","text":"get the name of the topic for a given state Parameters: Name Type Description Default state the state to get the topic for required relation_type optional, in case it has it's own topic '' Source code in event_stream/event_stream_base.py def get_topic_name ( self , state , relation_type = '' ): \"\"\"get the name of the topic for a given state Arguments: state: the state to get the topic for relation_type: optional, in case it has it's own topic \"\"\" result = self . event_string + self . state_separator + state # if a relation type is set and has is own topic if relation_type != '' and 'own_topic' in self . config_states [ state ] and relation_type in \\ self . config_states [ state ][ 'own_topic' ]: result = result + self . relation_type_separator + relation_type return result","title":"get_topic_name()"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase.get_topic_name_event","text":"this will resolve an event to it's respected kafka topic Parameters: Name Type Description Default key the event to be resolved required Source code in event_stream/event_stream_base.py def get_topic_name_event ( self , event ): \"\"\"this will resolve an event to it's respected kafka topic Arguments: key: the event to be resolved \"\"\" state = event . get ( 'state' ) relation_type = event . get ( 'relation_type' ) return self . get_topic_name ( state , relation_type )","title":"get_topic_name_event()"},{"location":"event_stream_base_ref/#event_stream.event_stream_base.EventStreamBase.resolve_event","text":"this will resolve an event to it's respected kafka topic Parameters: Name Type Description Default key the event to be resolved required Source code in event_stream/event_stream_base.py def resolve_event ( self , event ): \"\"\"this will resolve an event to it's respected kafka topic Arguments: key: the event to be resolved \"\"\" topic_name = self . build_topic_name ( event [ 'state' ], event [ 'relation_type' ]) if topic_name in self . topics : return topic_name logging . warning ( self . log + \"Unable to resolve event, topic_name %s not found\" % topic_name ) return False","title":"resolve_event()"},{"location":"event_stream_consumer_ref/","text":"EventStreamConsumer ( EventStreamBase ) a base consumer class for consuming from kafka, uses multiprocessing to share workload consume ( self ) consume messages and add them to a queue to share with the worker processes Source code in event_stream/event_stream_consumer.py def consume ( self ): \"\"\"consume messages and add them to a queue to share with the worker processes \"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . counter : self . counter = Value ( 'i' , 0 ) counter_time = 10 threading . Timer ( counter_time , throughput_statistics , args = [ self . counter , counter_time ]) . start () # Start worker processes # for i in range(self.process_number): # Process(target=self.on_message, args=(self.task_queue, )).start() pool = Pool ( self . process_number , self . worker , ( self . task_queue ,)) while self . running : try : for msg in self . consumer : logging . debug ( self . log + 'msg in consumer ' ) if self . counter : with self . counter . get_lock (): self . counter . value += 1 # logging.warning('msg in consumer %s' % msg.value) self . task_queue . put ( json . loads ( msg . value . decode ( 'utf-8' ))) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () pool . close () logging . warning ( self . log + \"Consumer shutdown\" ) create_consumer ( self ) create the consumer, connect to kafka Source code in event_stream/event_stream_consumer.py def create_consumer ( self ): \"\"\"create the consumer, connect to kafka \"\"\" logging . debug ( self . log + \"rt: %s \" % self . relation_type ) if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) # self.topic_name = 'tweets' logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) # consumer.topics() self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . debug ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ()) on_message ( self , json_msg ) the on message function to be implemented in own classes Parameters: Name Type Description Default json_msg the message to do stuff with required Source code in event_stream/event_stream_consumer.py def on_message ( self , json_msg ): \"\"\"the on message function to be implemented in own classes Arguments: json_msg: the message to do stuff with \"\"\" logging . debug ( self . log + \"on message\" ) start ( i = 0 ) staticmethod start the consumer Source code in event_stream/event_stream_consumer.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" esc = EventStreamConsumer ( i ) logging . debug ( EventStreamBase . log + 'Start %s ' % str ( i )) esc . consume () stop ( self ) stop the consumer Source code in event_stream/event_stream_consumer.py def stop ( self ): \"\"\"stop the consumer \"\"\" self . running = False logging . debug ( self . log + 'stop running consumer' ) worker ( self , queue ) worker function to get items from the queue Parameters: Name Type Description Default queue the queue required Source code in event_stream/event_stream_consumer.py def worker ( self , queue ): \"\"\"worker function to get items from the queue Arguments: queue: the queue \"\"\" logging . debug ( self . log + \"working %s \" % os . getpid ()) while self . running : time . sleep ( 0.005 ) try : item = queue . get () except queue . Empty : time . sleep ( 0.1 ) pass else : logging . debug ( self . log + \"got %s item\" % os . getpid ()) self . on_message ( item ) throughput_statistics ( v , time_delta , no_throughput_counter = 0 ) show and setup in own thread repeatedly how many events are processed restarts if counter of no throughput is 10 (10 timed eltas with no data processed) Parameters: Name Type Description Default v the value required time_delta time delta we wan't to monitor required no_throughput_counter counter of no throughput 0 Source code in event_stream/event_stream_consumer.py def throughput_statistics ( v , time_delta , no_throughput_counter = 0 ): \"\"\"show and setup in own thread repeatedly how many events are processed restarts if counter of no throughput is 10 (10 timed eltas with no data processed) Arguments: v: the value time_delta: time delta we wan't to monitor no_throughput_counter: counter of no throughput \"\"\" logging . warning ( \"THROUGHPUT: %d / %d \" % ( v . value , time_delta )) if v . value == 0 : no_throughput_counter += 1 if no_throughput_counter == 10 : sys . exit () # end so it will restart clean with v . get_lock (): v . value = 0 threading . Timer ( time_delta , throughput_statistics , args = [ v , time_delta , no_throughput_counter ]) . start ()","title":"event stream consumer"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer","text":"a base consumer class for consuming from kafka, uses multiprocessing to share workload","title":"EventStreamConsumer"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.consume","text":"consume messages and add them to a queue to share with the worker processes Source code in event_stream/event_stream_consumer.py def consume ( self ): \"\"\"consume messages and add them to a queue to share with the worker processes \"\"\" logging . warning ( self . log + \"start consume\" ) self . running = True if not self . consumer : self . create_consumer () if not self . counter : self . counter = Value ( 'i' , 0 ) counter_time = 10 threading . Timer ( counter_time , throughput_statistics , args = [ self . counter , counter_time ]) . start () # Start worker processes # for i in range(self.process_number): # Process(target=self.on_message, args=(self.task_queue, )).start() pool = Pool ( self . process_number , self . worker , ( self . task_queue ,)) while self . running : try : for msg in self . consumer : logging . debug ( self . log + 'msg in consumer ' ) if self . counter : with self . counter . get_lock (): self . counter . value += 1 # logging.warning('msg in consumer %s' % msg.value) self . task_queue . put ( json . loads ( msg . value . decode ( 'utf-8' ))) except Exception as exc : self . consumer . close () logging . error ( self . log + 'stream Consumer generated an exception: %s ' % exc ) logging . warning ( self . log + \"Consumer closed\" ) break # keep alive if self . running : return self . consume () pool . close () logging . warning ( self . log + \"Consumer shutdown\" )","title":"consume()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.create_consumer","text":"create the consumer, connect to kafka Source code in event_stream/event_stream_consumer.py def create_consumer ( self ): \"\"\"create the consumer, connect to kafka \"\"\" logging . debug ( self . log + \"rt: %s \" % self . relation_type ) if self . state == 'all' : self . topics = self . build_topic_list () if isinstance ( self . state , six . string_types ): self . state = [ self . state ] if isinstance ( self . relation_type , six . string_types ): self . relation_type = [ self . relation_type ] if not self . topics : self . topics = list () for state in self . state : for relation_type in self . relation_type : self . topics . append ( self . get_topic_name ( state = state , relation_type = relation_type )) # self.topic_name = 'tweets' logging . debug ( self . log + \"get consumer for topic: %s \" % self . topics ) # consumer.topics() self . consumer = KafkaConsumer ( group_id = self . group_id , bootstrap_servers = self . bootstrap_servers , api_version = self . api_version , consumer_timeout_ms = self . consumer_timeout_ms ) for topic in self . topics : logging . debug ( self . log + \"consumer subscribe: %s \" % topic ) self . consumer . subscribe ( topic ) logging . debug ( self . log + \"consumer subscribed to: %s \" % self . consumer . topics ())","title":"create_consumer()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.on_message","text":"the on message function to be implemented in own classes Parameters: Name Type Description Default json_msg the message to do stuff with required Source code in event_stream/event_stream_consumer.py def on_message ( self , json_msg ): \"\"\"the on message function to be implemented in own classes Arguments: json_msg: the message to do stuff with \"\"\" logging . debug ( self . log + \"on message\" )","title":"on_message()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.start","text":"start the consumer Source code in event_stream/event_stream_consumer.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" esc = EventStreamConsumer ( i ) logging . debug ( EventStreamBase . log + 'Start %s ' % str ( i )) esc . consume ()","title":"start()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.stop","text":"stop the consumer Source code in event_stream/event_stream_consumer.py def stop ( self ): \"\"\"stop the consumer \"\"\" self . running = False logging . debug ( self . log + 'stop running consumer' )","title":"stop()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.EventStreamConsumer.worker","text":"worker function to get items from the queue Parameters: Name Type Description Default queue the queue required Source code in event_stream/event_stream_consumer.py def worker ( self , queue ): \"\"\"worker function to get items from the queue Arguments: queue: the queue \"\"\" logging . debug ( self . log + \"working %s \" % os . getpid ()) while self . running : time . sleep ( 0.005 ) try : item = queue . get () except queue . Empty : time . sleep ( 0.1 ) pass else : logging . debug ( self . log + \"got %s item\" % os . getpid ()) self . on_message ( item )","title":"worker()"},{"location":"event_stream_consumer_ref/#event_stream.event_stream_consumer.throughput_statistics","text":"show and setup in own thread repeatedly how many events are processed restarts if counter of no throughput is 10 (10 timed eltas with no data processed) Parameters: Name Type Description Default v the value required time_delta time delta we wan't to monitor required no_throughput_counter counter of no throughput 0 Source code in event_stream/event_stream_consumer.py def throughput_statistics ( v , time_delta , no_throughput_counter = 0 ): \"\"\"show and setup in own thread repeatedly how many events are processed restarts if counter of no throughput is 10 (10 timed eltas with no data processed) Arguments: v: the value time_delta: time delta we wan't to monitor no_throughput_counter: counter of no throughput \"\"\" logging . warning ( \"THROUGHPUT: %d / %d \" % ( v . value , time_delta )) if v . value == 0 : no_throughput_counter += 1 if no_throughput_counter == 10 : sys . exit () # end so it will restart clean with v . get_lock (): v . value = 0 threading . Timer ( time_delta , throughput_statistics , args = [ v , time_delta , no_throughput_counter ]) . start ()","title":"throughput_statistics()"},{"location":"event_stream_producer_ref/","text":"EventStreamProducer ( EventStreamBase ) produce messages for kafka create_producer ( self ) create the producer Source code in event_stream/event_stream_producer.py def create_producer ( self ): \"\"\"create the producer \"\"\" self . producer = KafkaProducer ( bootstrap_servers = self . bootstrap_servers , api_version = self . api_version ) publish ( self , event ) publish an event Parameters: Name Type Description Default event the event which should be shared required Source code in event_stream/event_stream_producer.py def publish ( self , event ): \"\"\"publish an event Arguments: event: the event which should be shared \"\"\" topic_event = self . get_topic_name_event ( event ) if not self . producer : self . create_producer () value = event . get_json () # logging.warning(self.log + \"v %s\" % value) self . producer . send ( topic_event , value = value . encode ( 'utf-8' )) self . producer . flush () logging . debug ( self . log + 'Message published successfully to topic %s ' % topic_event )","title":"event stream producer"},{"location":"event_stream_producer_ref/#event_stream.event_stream_producer.EventStreamProducer","text":"produce messages for kafka","title":"EventStreamProducer"},{"location":"event_stream_producer_ref/#event_stream.event_stream_producer.EventStreamProducer.create_producer","text":"create the producer Source code in event_stream/event_stream_producer.py def create_producer ( self ): \"\"\"create the producer \"\"\" self . producer = KafkaProducer ( bootstrap_servers = self . bootstrap_servers , api_version = self . api_version )","title":"create_producer()"},{"location":"event_stream_producer_ref/#event_stream.event_stream_producer.EventStreamProducer.publish","text":"publish an event Parameters: Name Type Description Default event the event which should be shared required Source code in event_stream/event_stream_producer.py def publish ( self , event ): \"\"\"publish an event Arguments: event: the event which should be shared \"\"\" topic_event = self . get_topic_name_event ( event ) if not self . producer : self . create_producer () value = event . get_json () # logging.warning(self.log + \"v %s\" % value) self . producer . send ( topic_event , value = value . encode ( 'utf-8' )) self . producer . flush () logging . debug ( self . log + 'Message published successfully to topic %s ' % topic_event )","title":"publish()"}]}